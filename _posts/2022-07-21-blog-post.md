
## Post Machine Learning Thoughts

Since my last blog post, I’ve learned about classification/regression
trees and ensemble models. I’ve learned the differences between
supervised and unsupervised learning and how to compare different models
by fitting testing data on it. By far, I think the coolest method has to
be random forests, as it takes into account both classification and
regression and constructs many decision trees when training data. The
idea for classification is that the output the random forest gives out
will be the most prevalent class among the trees, while for regression
it’s the mean or average prediction of the trees. We will be looking at
the `quakes` data set, with our response being `magnitude`. Below, we’ll
run a random forest to show how it works. Let’s import our data first.

``` r
library(tidyverse)
library(dplyr)
library(caret)
library(stats)
library(ggplot2)
library(ggpubr)
data("quakes")
print(quakes)
```

    ##        lat   long depth mag stations
    ## 1   -20.42 181.62   562 4.8       41
    ## 2   -20.62 181.03   650 4.2       15
    ## 3   -26.00 184.10    42 5.4       43
    ## 4   -17.97 181.66   626 4.1       19
    ## 5   -20.42 181.96   649 4.0       11
    ## 6   -19.68 184.31   195 4.0       12
    ## 7   -11.70 166.10    82 4.8       43
    ## 8   -28.11 181.93   194 4.4       15
    ## 9   -28.74 181.74   211 4.7       35
    ## 10  -17.47 179.59   622 4.3       19
    ## 11  -21.44 180.69   583 4.4       13
    ## 12  -12.26 167.00   249 4.6       16
    ## 13  -18.54 182.11   554 4.4       19
    ## 14  -21.00 181.66   600 4.4       10
    ## 15  -20.70 169.92   139 6.1       94
    ## 16  -15.94 184.95   306 4.3       11
    ## 17  -13.64 165.96    50 6.0       83
    ## 18  -17.83 181.50   590 4.5       21
    ## 19  -23.50 179.78   570 4.4       13
    ## 20  -22.63 180.31   598 4.4       18
    ## 21  -20.84 181.16   576 4.5       17
    ## 22  -10.98 166.32   211 4.2       12
    ## 23  -23.30 180.16   512 4.4       18
    ## 24  -30.20 182.00   125 4.7       22
    ## 25  -19.66 180.28   431 5.4       57
    ## 26  -17.94 181.49   537 4.0       15
    ## 27  -14.72 167.51   155 4.6       18
    ## 28  -16.46 180.79   498 5.2       79
    ## 29  -20.97 181.47   582 4.5       25
    ## 30  -19.84 182.37   328 4.4       17
    ## 31  -22.58 179.24   553 4.6       21
    ## 32  -16.32 166.74    50 4.7       30
    ## 33  -15.55 185.05   292 4.8       42
    ## 34  -23.55 180.80   349 4.0       10
    ## 35  -16.30 186.00    48 4.5       10
    ## 36  -25.82 179.33   600 4.3       13
    ## 37  -18.73 169.23   206 4.5       17
    ## 38  -17.64 181.28   574 4.6       17
    ## 39  -17.66 181.40   585 4.1       17
    ## 40  -18.82 169.33   230 4.4       11
    ## 41  -37.37 176.78   263 4.7       34
    ## 42  -15.31 186.10    96 4.6       32
    ## 43  -24.97 179.82   511 4.4       23
    ## 44  -15.49 186.04    94 4.3       26
    ## 45  -19.23 169.41   246 4.6       27
    ## 46  -30.10 182.30    56 4.9       34
    ## 47  -26.40 181.70   329 4.5       24
    ## 48  -11.77 166.32    70 4.4       18
    ## 49  -24.12 180.08   493 4.3       21
    ## 50  -18.97 185.25   129 5.1       73
    ## 51  -18.75 182.35   554 4.2       13
    ## 52  -19.26 184.42   223 4.0       15
    ## 53  -22.75 173.20    46 4.6       26
    ## 54  -21.37 180.67   593 4.3       13
    ## 55  -20.10 182.16   489 4.2       16
    ## 56  -19.85 182.13   562 4.4       31
    ## 57  -22.70 181.00   445 4.5       17
    ## 58  -22.06 180.60   584 4.0       11
    ## 59  -17.80 181.35   535 4.4       23
    ## 60  -24.20 179.20   530 4.3       12
    ## 61  -20.69 181.55   582 4.7       35
    ## 62  -21.16 182.40   260 4.1       12
    ## 63  -13.82 172.38   613 5.0       61
    ## 64  -11.49 166.22    84 4.6       32
    ## 65  -20.68 181.41   593 4.9       40
    ## 66  -17.10 184.93   286 4.7       25
    ## 67  -20.14 181.60   587 4.1       13
    ## 68  -21.96 179.62   627 5.0       45
    ## 69  -20.42 181.86   530 4.5       27
    ## 70  -15.46 187.81    40 5.5       91
    ## 71  -15.31 185.80   152 4.0       11
    ## 72  -19.86 184.35   201 4.5       30
    ## 73  -11.55 166.20    96 4.3       14
    ## 74  -23.74 179.99   506 5.2       75
    ## 75  -17.70 181.23   546 4.4       35
    ## 76  -23.54 180.04   564 4.3       15
    ## 77  -19.21 184.70   197 4.1       11
    ## 78  -12.11 167.06   265 4.5       23
    ## 79  -21.81 181.71   323 4.2       15
    ## 80  -28.98 181.11   304 5.3       60
    ## 81  -34.02 180.21    75 5.2       65
    ## 82  -23.84 180.99   367 4.5       27
    ## 83  -19.57 182.38   579 4.6       38
    ## 84  -20.12 183.40   284 4.3       15
    ## 85  -17.70 181.70   450 4.0       11
    ## 86  -19.66 184.31   170 4.3       15
    ## 87  -21.50 170.50   117 4.7       32
    ## 88  -23.64 179.96   538 4.5       26
    ## 89  -15.43 186.30   123 4.2       16
    ## 90  -15.41 186.44    69 4.3       42
    ## 91  -15.48 167.53   128 5.1       61
    ## 92  -13.36 167.06   236 4.7       22
    ## 93  -20.64 182.02   497 5.2       64
    ## 94  -19.72 169.71   271 4.2       14
    ## 95  -15.44 185.26   224 4.2       21
    ## 96  -19.73 182.40   375 4.0       18
    ## 97  -27.24 181.11   365 4.5       21
    ## 98  -18.16 183.41   306 5.2       54
    ## 99  -13.66 166.54    50 5.1       45
    ## 100 -24.57 179.92   484 4.7       33
    ## 101 -16.98 185.61   108 4.1       12
    ## 102 -26.20 178.41   583 4.6       25
    ## 103 -21.88 180.39   608 4.7       30
    ## 104 -33.00 181.60    72 4.7       22
    ## 105 -21.33 180.69   636 4.6       29
    ## 106 -19.44 183.50   293 4.2       15
    ## 107 -34.89 180.60    42 4.4       25
    ## 108 -20.24 169.49   100 4.6       22
    ## 109 -22.55 185.90    42 5.7       76
    ## 110 -36.95 177.81   146 5.0       35
    ## 111 -15.75 185.23   280 4.5       28
    ## 112 -16.85 182.31   388 4.2       14
    ## 113 -19.06 182.45   477 4.0       16
    ## 114 -26.11 178.30   617 4.8       39
    ## 115 -26.20 178.35   606 4.4       21
    ## 116 -26.13 178.31   609 4.2       25
    ## 117 -13.66 172.23    46 5.3       67
    ## 118 -13.47 172.29    64 4.7       14
    ## 119 -14.60 167.40   178 4.8       52
    ## 120 -18.96 169.48   248 4.2       13
    ## 121 -14.65 166.97    82 4.8       28
    ## 122 -19.90 178.90    81 4.3       11
    ## 123 -22.05 180.40   606 4.7       27
    ## 124 -19.22 182.43   571 4.5       23
    ## 125 -31.24 180.60   328 4.4       18
    ## 126 -17.93 167.89    49 5.1       43
    ## 127 -19.30 183.84   517 4.2       21
    ## 128 -26.53 178.57   600 5.0       69
    ## 129 -27.72 181.70    94 4.8       59
    ## 130 -19.19 183.51   307 4.3       19
    ## 131 -17.43 185.43   189 4.5       22
    ## 132 -17.05 181.22   527 4.2       24
    ## 133 -19.52 168.98    63 4.5       21
    ## 134 -23.71 180.30   510 4.6       30
    ## 135 -21.30 180.82   624 4.3       14
    ## 136 -16.24 168.02    53 4.7       12
    ## 137 -16.14 187.32    42 5.1       68
    ## 138 -23.95 182.80   199 4.6       14
    ## 139 -25.20 182.60   149 4.9       31
    ## 140 -18.84 184.16   210 4.2       17
    ## 141 -12.66 169.46   658 4.6       43
    ## 142 -20.65 181.40   582 4.0       14
    ## 143 -13.23 167.10   220 5.0       46
    ## 144 -29.91 181.43   205 4.4       34
    ## 145 -14.31 173.50   614 4.2       23
    ## 146 -20.10 184.40   186 4.2       10
    ## 147 -17.80 185.17    97 4.4       22
    ## 148 -21.27 173.49    48 4.9       42
    ## 149 -23.58 180.17   462 5.3       63
    ## 150 -17.90 181.50   573 4.0       19
    ## 151 -23.34 184.50    56 5.7      106
    ## 152 -15.56 167.62   127 6.4      122
    ## 153 -23.83 182.56   229 4.3       24
    ## 154 -11.80 165.80   112 4.2       20
    ## 155 -15.54 167.68   140 4.7       16
    ## 156 -20.65 181.32   597 4.7       39
    ## 157 -11.75 166.07    69 4.2       14
    ## 158 -24.81 180.00   452 4.3       19
    ## 159 -20.90 169.84    93 4.9       31
    ## 160 -11.34 166.24   103 4.6       30
    ## 161 -17.98 180.50   626 4.1       19
    ## 162 -24.34 179.52   504 4.8       34
    ## 163 -13.86 167.16   202 4.6       30
    ## 164 -35.56 180.20    42 4.6       32
    ## 165 -35.48 179.90    59 4.8       35
    ## 166 -34.20 179.43    40 5.0       37
    ## 167 -26.00 182.12   205 5.6       98
    ## 168 -19.89 183.84   244 5.3       73
    ## 169 -23.43 180.00   553 4.7       41
    ## 170 -18.89 169.42   239 4.5       27
    ## 171 -17.82 181.83   640 4.3       24
    ## 172 -25.68 180.34   434 4.6       41
    ## 173 -20.20 180.90   627 4.1       11
    ## 174 -15.20 184.68    99 4.1       14
    ## 175 -15.03 182.29   399 4.1       10
    ## 176 -32.22 180.20   216 5.7       90
    ## 177 -22.64 180.64   544 5.0       50
    ## 178 -17.42 185.16   206 4.5       22
    ## 179 -17.84 181.48   542 4.1       20
    ## 180 -15.02 184.24   339 4.6       27
    ## 181 -18.04 181.75   640 4.5       47
    ## 182 -24.60 183.50    67 4.3       25
    ## 183 -19.88 184.30   161 4.4       17
    ## 184 -20.30 183.00   375 4.2       15
    ## 185 -20.45 181.85   534 4.1       14
    ## 186 -17.67 187.09    45 4.9       62
    ## 187 -22.30 181.90   309 4.3       11
    ## 188 -19.85 181.85   576 4.9       54
    ## 189 -24.27 179.88   523 4.6       24
    ## 190 -15.85 185.13   290 4.6       29
    ## 191 -20.02 184.09   234 5.3       71
    ## 192 -18.56 169.31   223 4.7       35
    ## 193 -17.87 182.00   569 4.6       12
    ## 194 -24.08 179.50   605 4.1       21
    ## 195 -32.20 179.61   422 4.6       41
    ## 196 -20.36 181.19   637 4.2       23
    ## 197 -23.85 182.53   204 4.6       27
    ## 198 -24.00 182.75   175 4.5       14
    ## 199 -20.41 181.74   538 4.3       31
    ## 200 -17.72 180.30   595 5.2       74
    ##  [ reached 'max' / getOption("max.print") -- omitted 800 rows ]

Now, let’s split up our data into a training and test set.

``` r
set.seed(123)
train <- sample(1:nrow(quakes), size = nrow(quakes)*.75)
test <- dplyr::setdiff(1:nrow(quakes), train)
quaketrain <- quakes[train, ]
quaketest <- quakes[test, ]
```

Finally, let’s fit a random forest model with this data.

``` r
trctrl <- trainControl(method = "repeatedcv", number=10, repeats=3)
tgrf <- expand.grid(mtry=c(1:15))
rforest <- train(mag ~ ., 
               method='rf', 
               trControl=trctrl, 
               data=quaketrain, 
               preProcess=c("center", "scale"),
               tuneGrid=tgrf)
#predict the values for our response variable and compare it to our testing data. 
rforest_pred <- predict(rforest, newdata = select(quaketest, -mag))
rforest_pred
```

    ##        1        3        7        9       12       15       18       21       22       25       27       28 
    ## 4.687333 4.952603 4.730030 4.549047 4.302617 5.442707 4.207340 4.402267 4.224790 4.890513 4.450400 5.198437 
    ##       32       35       42       43       44       47       60       62       63       66       70       73 
    ## 4.723413 4.474143 4.609973 4.380430 4.508950 4.364400 4.256390 4.250723 5.102593 4.497213 5.411153 4.495167 
    ##       75       82       86       92       97      101      102      103      107      109      126      133 
    ## 4.459867 4.437517 4.165180 4.453540 4.351843 4.307313 4.441117 4.592050 4.678937 5.240237 4.920303 4.758380 
    ##      140      142      144      145      146      147      149      150      154      156      157      174 
    ## 4.171313 4.303050 4.579977 4.441020 4.195847 4.364817 5.041537 4.163647 4.580550 4.666150 4.685557 4.289420 
    ##      176      182      183      192      194      198      202      208      213      214      215      216 
    ## 5.368573 4.632250 4.296350 4.702217 4.327320 4.340717 4.099487 4.241713 4.352427 5.280750 4.279120 4.348410 
    ##      233      245      247      249      253      254      257      269      272      283      285      288 
    ## 4.113353 4.882933 4.312567 5.088757 4.703470 4.848183 4.590807 4.509577 4.603707 4.494897 4.230110 4.734413 
    ##      293      296      300      307      313      314      321      325      329      335      345      350 
    ## 4.561163 4.734440 4.443893 4.222810 5.291053 4.323433 4.635267 5.096677 4.327837 5.072263 4.402800 4.469250 
    ##      353      354      356      359      360      366      367      368      369      370      375      380 
    ## 4.315697 5.344100 4.904200 4.591700 4.506410 4.578947 4.875593 4.345817 4.187097 4.365763 4.275017 5.372710 
    ##      383      385      387      408      410      411      416      423      425      432      436      439 
    ## 4.527110 4.904913 4.183350 4.631663 4.475617 4.636527 4.962873 4.342223 4.979867 4.214983 4.386060 4.480537 
    ##      449      453      454      460      467      472      474      482      484      485      486      489 
    ## 5.555413 4.689027 4.331083 4.665833 4.575850 4.612733 4.697787 4.144347 4.815160 4.147653 5.346080 4.722603 
    ##      491      495      496      497      502      506      511      514      515      517      518      520 
    ## 4.542597 4.129477 5.332783 4.553463 4.552193 4.246713 4.825933 4.577310 4.196643 4.577090 4.510777 4.807570 
    ##      521      525      531      542      543      546      556      563      565      568      572      576 
    ## 4.574217 5.443760 5.297123 4.498020 4.574527 4.557713 4.391237 4.693490 4.352600 5.134827 4.879613 4.553637 
    ##      579      580      583      584      586      587      592      594      599      607      616      622 
    ## 5.184003 4.956007 5.106617 4.277760 4.942757 4.748613 4.299590 4.876697 4.537430 4.606243 4.886787 4.477477 
    ##      628      631      641      642      643      653      656      669      674      675      683      684 
    ## 4.381427 4.445117 4.410513 4.656923 4.793967 5.598113 4.698210 4.758420 4.540657 4.550473 4.554330 4.169553 
    ##      689      693      699      701      708      713      715      728      730      731      735      736 
    ## 5.058460 4.374690 4.527857 4.422640 5.184267 4.984777 4.170943 4.521560 4.302330 4.688977 4.419903 4.661430 
    ##      737      740      743      748      749      756      758      759      763      772      773      776 
    ## 4.342810 4.408877 4.597117 4.563617 4.404827 4.250600 5.310567 4.914703 4.091117 4.108720 4.294507 4.285373 
    ##      786      787      791      793      795      796      799      806      825      826      827      828 
    ## 4.754187 5.307823 4.214607 4.232223 4.590540 4.387533 4.492227 4.347627 4.529870 4.244077 4.277200 4.490863 
    ##      829      830      833      839      848      849      850      855      856      866      868      874 
    ## 4.756830 4.324850 4.200137 4.973340 4.475610 4.915973 5.219147 4.473130 4.135440 4.591973 4.416863 4.434710 
    ##      875      879      884      892      896      907      909      914      919      921      924      929 
    ## 4.128253 4.621270 4.504337 4.312087 4.331953 4.630630 4.387553 4.571160 4.346870 5.333647 4.217497 4.600847 
    ##      936      939      945      946      950      952      963      964      967      970      971      972 
    ## 5.605517 4.254123 4.743047 4.601767 4.849870 5.426770 4.536417 4.329540 4.791373 4.936973 4.275787 5.213037 
    ##      973      978      983      984      985      989      993      995      997      998 
    ## 4.531953 4.525340 4.959653 4.532310 4.573467 4.176533 5.168150 4.453193 4.863627 4.547720
